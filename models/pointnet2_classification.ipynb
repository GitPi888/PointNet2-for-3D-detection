{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a6be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointnet2_utils import PointNetSetAbstraction, PointNetSetAbstractionMsg, PointNetFeaturePropagation, Attention, GLUBlock, LightHead\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import collections\n",
    "from scipy.linalg import expm,norm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2bd9db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904e55e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions:\n",
    "def farthest_point_sample(xyz, npoint):\n",
    "    batch_size, n, _ = xyz.shape\n",
    "    device = xyz.device\n",
    "    centroids = torch.zeros(batch_size, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(batch_size, n).to(device) * 1e10\n",
    "    farthest = torch.randint(0, n, (batch_size,), dtype=torch.long).to(device)\n",
    "    batch_indices = torch.arange(batch_size, dtype=torch.long).to(device)\n",
    "\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].unsqueeze(1)  # [B, 1, 3]\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1)  # Squared distance [B, N]\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = torch.max(distance, -1)[1]  # Index of the farthest point\n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def gather_points(xyz, idx):\n",
    "    batch_size, n, _ = xyz.shape\n",
    "\n",
    "    if idx.dim() == 2:  # Case 1: [B, npoint]\n",
    "        _, npoint = idx.shape\n",
    "        idx = idx.view(-1)  # Flatten indices for batch processing\n",
    "        gathered_xyz = xyz.reshape(batch_size * n, -1)[idx, :]  # Gather points\n",
    "        gathered_xyz = gathered_xyz.reshape(batch_size, npoint, -1)  # Reshape back\n",
    "    elif idx.dim() == 3:  # Case 2: [B, npoint, nsample]\n",
    "        _, npoint, nsample = idx.shape\n",
    "        idx_base = torch.arange(0, batch_size, device=xyz.device).view(-1, 1, 1) * n\n",
    "        idx = idx + idx_base  # Flatten indices for batch processing\n",
    "        idx = idx.reshape(-1)  # Flatten completely\n",
    "        gathered_xyz = xyz.reshape(batch_size * n, -1)[idx, :]  # Gather points\n",
    "        gathered_xyz = gathered_xyz.reshape(batch_size, npoint, nsample, -1)  # Reshape back\n",
    "\n",
    "    return gathered_xyz\n",
    "\n",
    "\n",
    "def query_and_group(xyz, new_xyz, points, radius, nsample):\n",
    "    B, N, _ = xyz.shape\n",
    "    _, npoint, _ = new_xyz.shape\n",
    "\n",
    "    # Compute squared distances between sampled points and all points\n",
    "    sqrdists = square_distance(new_xyz, xyz)  # [B, npoint, N]\n",
    "\n",
    "    # Find indices of the nearest neighbors\n",
    "    group_idx = sqrdists.argsort(dim=-1)[:, :, :nsample]  # [B, npoint, nsample]\n",
    "\n",
    "    # Gather the grouped xyz coordinates\n",
    "    grouped_xyz = gather_points(xyz, group_idx)  # [B, npoint, nsample, 3]\n",
    "    grouped_xyz = grouped_xyz - new_xyz.unsqueeze(2)  # Local coordinates [B, npoint, nsample, 3]\n",
    "\n",
    "    if points is not None:\n",
    "        grouped_points = gather_points(points.transpose(1, 2), group_idx).permute(0, 3, 2, 1)  # [B, C, nsample, npoint]\n",
    "        new_points = torch.cat([grouped_xyz.permute(0, 3, 2, 1), grouped_points], dim=1)  # [B, C+3, nsample, npoint]\n",
    "    else:\n",
    "        new_points = grouped_xyz.permute(0, 3, 2, 1)  # [B, 3, nsample, npoint]\n",
    "\n",
    "    return new_points\n",
    "\n",
    "\n",
    "def square_distance(src, dst):\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))  # [B, N, M]\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist\n",
    "\n",
    "\n",
    "class SetAbstraction(nn.Module):\n",
    "    def __init__(self, npoint, radius, nsample, in_channel, mlp):\n",
    "        super(SetAbstraction, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius = radius\n",
    "        self.nsample = nsample\n",
    "\n",
    "        # Include x, y, z coordinates in the input channel count\n",
    "        last_channel = in_channel + 3  # Add (x, y, z)\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))  # Conv2D expects [B, C_in, nsample, npoint]\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        if self.npoint is not None:\n",
    "            idx = farthest_point_sample(xyz, self.npoint)  # [B, npoint]\n",
    "            new_xyz = gather_points(xyz, idx)  # [B, npoint, 3]\n",
    "        else:\n",
    "            new_xyz = xyz  # Use all points if npoint is None\n",
    "\n",
    "        grouped_points = query_and_group(xyz, new_xyz, points, self.radius, self.nsample)  # [B, C+3, nsample, npoint]\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            grouped_points = F.relu(self.mlp_bns[i](conv(grouped_points)))  # [B, out_channel, nsample, npoint]\n",
    "\n",
    "        # Max pooling over nsample dimension\n",
    "        new_points = torch.max(grouped_points, 2)[0]  # [B, mlp[-1], npoint]\n",
    "        return new_xyz, new_points\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Lớp self-attention cho đặc trưng điểm trong point cloud.\n",
    "    Dùng sau mỗi tầng Set Abstraction để tăng hiệu suất mô hình học hình học.\n",
    "    Input: [B, C, N] (batch, channel, num_points)\n",
    "    Output: [B, C, N]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, heads=4):\n",
    "        super(Attention, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.heads = heads\n",
    "        self.dk = in_channels // heads\n",
    "        assert in_channels % heads == 0, \"in_channels phải chia hết cho số heads\"\n",
    "        self.query = nn.Conv1d(in_channels, in_channels, 1)\n",
    "        self.key = nn.Conv1d(in_channels, in_channels, 1)\n",
    "        self.value = nn.Conv1d(in_channels, in_channels, 1)\n",
    "        self.proj = nn.Conv1d(in_channels, in_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, N]\n",
    "        B, C, N = x.shape\n",
    "        Q = self.query(x).view(B, self.heads, self.dk, N)  # [B, heads, dk, N]\n",
    "        K = self.key(x).view(B, self.heads, self.dk, N)\n",
    "        V = self.value(x).view(B, self.heads, self.dk, N)\n",
    "        attn = torch.einsum('bhdk,bhdk->bhdn', Q, K) / (self.dk ** 0.5)  # [B, heads, N, N]\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        out = torch.einsum('bhdn,bhdn->bhdk', attn, V)  # [B, heads, dk, N]\n",
    "        out = out.contiguous().view(B, C, N)\n",
    "        out = self.proj(out)\n",
    "        return out + x  # residual\n",
    "    \n",
    "\n",
    "class GLUBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear_main = nn.Linear(in_dim, out_dim)\n",
    "        self.linear_gate = nn.Linear(in_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_main(x) * torch.sigmoid(self.linear_gate(x))\n",
    "\n",
    "# Define PointNet++ model\n",
    "class PointNetPlusPlus(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PointNetPlusPlus, self).__init__()\n",
    "\n",
    "        # Set Abstraction layers\n",
    "        self.sa1 = SetAbstraction(npoint=512, radius=0.2, nsample=32, in_channel=0, mlp=[64, 64, 128])\n",
    "        self.sa2 = SetAbstraction(npoint=128, radius=0.4, nsample=64, in_channel=128, mlp=[128, 128, 256])\n",
    "        self.sa3 = SetAbstraction(npoint=None, radius=None, nsample=None, in_channel=256, mlp=[256, 512, 1024])\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        self.light_head = LightHead(in_dim=1024, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, xyz):\n",
    "        batch_size, _, _ = xyz.shape\n",
    "\n",
    "        # Hierarchical feature extraction\n",
    "        l1_xyz, l1_points = self.sa1(xyz, None)       # Layer 1: [B, 512, 128]\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)  # Layer 2: [B, 128, 256]\n",
    "        _, l3_points = self.sa3(l2_xyz, l2_points)    # Layer 3: [B, 1024, npoint]\n",
    "        # Fully connected layers\n",
    "        x = self.light_head(l3_points)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19a1f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_labels(label_dir):\n",
    "    unique_labels = set()\n",
    "    for label_file in os.listdir(label_dir):\n",
    "        if label_file.endswith('.txt'):\n",
    "            with open(os.path.join(label_dir, label_file), 'r') as file:\n",
    "                for line in file:\n",
    "                    parts = line.strip().split()\n",
    "                    unique_labels.add(parts[0])  # Add the label (Class)\n",
    "    return sorted(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e83cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KittiPointCloudDataset(Dataset):\n",
    "    def __init__(self, velodyne_dir, label_dir, label_to_id, num_points=1024):\n",
    "        self.velodyne_dir = velodyne_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.label_to_id = label_to_id\n",
    "        self.num_points = num_points\n",
    "\n",
    "\n",
    "        # List all velodyne files and label files\n",
    "        self.velodyne_files = sorted([f for f in os.listdir(velodyne_dir) if f.endswith('.bin')])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.txt')])\n",
    "\n",
    "        # Ensure point clouds and labels align\n",
    "        assert len(self.velodyne_files) == len(self.label_files), \"Mismatch between point clouds and labels\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.velodyne_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load point cloud\n",
    "        pc_file = os.path.join(self.velodyne_dir, self.velodyne_files[idx])\n",
    "        point_cloud = self.load_point_cloud(pc_file)\n",
    "\n",
    "        # Downsample or pad point cloud\n",
    "        if len(point_cloud) > self.num_points:\n",
    "            idxs = np.random.choice(len(point_cloud), self.num_points, replace=False)\n",
    "        else:\n",
    "            idxs = np.random.choice(len(point_cloud), self.num_points, replace=True)\n",
    "        point_cloud = point_cloud[idxs]\n",
    "        # Load and parse labels\n",
    "        label_file = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        labels = self.parse_labels(label_file)\n",
    "\n",
    "        return torch.tensor(point_cloud, dtype=torch.float32), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def load_point_cloud(self, file_path):\n",
    "        \"\"\"Load point cloud from .bin file\"\"\"\n",
    "        point_cloud = np.fromfile(file_path, dtype=np.float32).reshape(-1, 4)  # x, y, z, intensity\n",
    "        return point_cloud[:, :3]  # Use only x, y, z\n",
    "\n",
    "    def parse_labels(self, label_file):\n",
    "        \"\"\"Parse label file and map labels to IDs using label_to_id\"\"\"\n",
    "        with open(label_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                obj_class = parts[0]\n",
    "                if obj_class in self.label_to_id:\n",
    "                    return self.label_to_id[obj_class]\n",
    "        return self.label_to_id.get('DontCare', 0)  # Default to DontCare (0) if label is missing\n",
    "\n",
    "class KittiTestDataset(Dataset):\n",
    "    def __init__(self, velodyne_dir, num_points=1024):\n",
    "        self.velodyne_dir = velodyne_dir\n",
    "        self.num_points = num_points\n",
    "\n",
    "        # List all velodyne files\n",
    "        self.velodyne_files = sorted([f for f in os.listdir(velodyne_dir) if f.endswith('.bin')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.velodyne_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load point cloud\n",
    "        pc_file = os.path.join(self.velodyne_dir, self.velodyne_files[idx])\n",
    "        point_cloud = self.load_point_cloud(pc_file)\n",
    "\n",
    "        # Downsample or pad point cloud\n",
    "        if len(point_cloud) > self.num_points:\n",
    "            idxs = np.random.choice(len(point_cloud), self.num_points, replace=False)\n",
    "        else:\n",
    "            idxs = np.random.choice(len(point_cloud), self.num_points, replace=True)\n",
    "        point_cloud = point_cloud[idxs]\n",
    "\n",
    "        return torch.tensor(point_cloud, dtype=torch.float32)\n",
    "\n",
    "    def load_point_cloud(self, file_path):\n",
    "        \"\"\"Load point cloud from .bin file\"\"\"\n",
    "        point_cloud = np.fromfile(file_path, dtype=np.float32).reshape(-1, 4)  # x, y, z, intensity\n",
    "        return point_cloud[:, :3]  # Use only x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c94cb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted label mapping: {'Car': 0, 'Cyclist': 1, 'DontCare': 2, 'Misc': 3, 'Pedestrian': 4, 'Person_sitting': 5, 'Tram': 6, 'Truck': 7, 'Van': 8}\n"
     ]
    }
   ],
   "source": [
    "# Paths to KITTI directories\n",
    "velodyne_dir = r\"E:\\Storange\\Python\\Point_cloud\\data\\archive\\training\\velodyne_subset\"\n",
    "label_dir = r\"E:\\Storange\\Python\\Point_cloud\\data\\archive\\training\\label_2_subset\"\n",
    "\n",
    "unique_labels = extract_unique_labels(label_dir)\n",
    "label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "print(\"Extracted label mapping:\", label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb89ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    points, labels = zip(*batch)\n",
    "    return torch.stack(points), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03f903a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train dataset and DataLoader\n",
    "train_dataset = KittiPointCloudDataset(velodyne_dir, label_dir, label_to_id, num_points=1024)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: collate_fn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5173ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1086a6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024, 3])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    points, labels = batch\n",
    "    print(points.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0ddf3fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device kernel image is invalid\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m points, labels \u001b[38;5;241m=\u001b[39m points\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(points)   \u001b[38;5;66;03m# No need for reshaping\u001b[39;00m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 162\u001b[0m, in \u001b[0;36mPointNetPlusPlus.forward\u001b[1;34m(self, xyz)\u001b[0m\n\u001b[0;32m    159\u001b[0m batch_size, _, _ \u001b[38;5;241m=\u001b[39m xyz\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Hierarchical feature extraction\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m l1_xyz, l1_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa1(xyz, \u001b[38;5;28;01mNone\u001b[39;00m)       \u001b[38;5;66;03m# Layer 1: [B, 512, 128]\u001b[39;00m\n\u001b[0;32m    163\u001b[0m l2_xyz, l2_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa2(l1_xyz, l1_points)  \u001b[38;5;66;03m# Layer 2: [B, 128, 256]\u001b[39;00m\n\u001b[0;32m    164\u001b[0m _, l3_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa3(l2_xyz, l2_points)    \u001b[38;5;66;03m# Layer 3: [B, 1024, npoint]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 90\u001b[0m, in \u001b[0;36mSetAbstraction.forward\u001b[1;34m(self, xyz, points)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xyz, points):\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m         idx \u001b[38;5;241m=\u001b[39m farthest_point_sample(xyz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpoint)  \u001b[38;5;66;03m# [B, npoint]\u001b[39;00m\n\u001b[0;32m     91\u001b[0m         new_xyz \u001b[38;5;241m=\u001b[39m gather_points(xyz, idx)  \u001b[38;5;66;03m# [B, npoint, 3]\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m, in \u001b[0;36mfarthest_point_sample\u001b[1;34m(xyz, npoint)\u001b[0m\n\u001b[0;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m xyz\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m      5\u001b[0m centroids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, npoint, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 6\u001b[0m distance \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, n)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e10\u001b[39m\n\u001b[0;32m      7\u001b[0m farthest \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, n, (batch_size,), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device kernel image is invalid\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "from torch import optim\n",
    "\n",
    "num_classes = len(label_to_id)\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PointNetPlusPlus(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "      points, labels = points.to(device), labels.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(points)   # No need for reshaping\n",
    "\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "      total += labels.size(0)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(train_loader):.4f}, Accuracy: {correct/total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79107082",
   "metadata": {},
   "source": [
    "<h2> Testing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = r\"E:\\Storange\\Python\\Point_cloud\\data\\archive\\testing\\velodyne\"\n",
    "test_dataset = KittiPointCloudDataset(test_path, num_points=1024)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store predictions\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for points in test_loader:\n",
    "        points = points.to(device)  # Send points to GPU if available\n",
    "        outputs = model(points)  # Get predictions\n",
    "        _, predicted_classes = torch.max(outputs, 1)  # Predicted class indices\n",
    "        predictions.append(predicted_classes.cpu().numpy())  # Store predictions\n",
    "\n",
    "# Flatten predictions into a single array\n",
    "predictions = np.concatenate(predictions, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
