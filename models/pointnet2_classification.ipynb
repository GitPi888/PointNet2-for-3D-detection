{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a6be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointnet2_utils import PointNetSetAbstraction, PointNetSetAbstractionMsg, PointNetFeaturePropagation, Attention, GLUBlock, LightHead\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import collections\n",
    "from scipy.linalg import expm,norm\n",
    "import glob\n",
    "import open3d as o3d\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f3f1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: PHUC\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "print(\"Running on:\", socket.gethostname())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92fcbb6",
   "metadata": {},
   "source": [
    "<h2> Build PointNet++ </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904e55e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions:\n",
    "def farthest_point_sample(xyz, npoint):\n",
    "    batch_size, n, _ = xyz.shape\n",
    "    device = xyz.device\n",
    "    centroids = torch.zeros(batch_size, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(batch_size, n).to(device) * 1e10\n",
    "    farthest = torch.randint(0, n, (batch_size,), dtype=torch.long).to(device)\n",
    "    batch_indices = torch.arange(batch_size, dtype=torch.long).to(device)\n",
    "\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].unsqueeze(1)  # [B, 1, 3]\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1)  # Squared distance [B, N]\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = torch.max(distance, -1)[1]  # Index of the farthest point\n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def gather_points(xyz, idx):\n",
    "    batch_size, n, _ = xyz.shape\n",
    "\n",
    "    if idx.dim() == 2:  # Case 1: [B, npoint]\n",
    "        _, npoint = idx.shape\n",
    "        idx = idx.view(-1)  # Flatten indices for batch processing\n",
    "        gathered_xyz = xyz.reshape(batch_size * n, -1)[idx, :]  # Gather points\n",
    "        gathered_xyz = gathered_xyz.reshape(batch_size, npoint, -1)  # Reshape back\n",
    "    elif idx.dim() == 3:  # Case 2: [B, npoint, nsample]\n",
    "        _, npoint, nsample = idx.shape\n",
    "        idx_base = torch.arange(0, batch_size, device=xyz.device).view(-1, 1, 1) * n\n",
    "        idx = idx + idx_base  # Flatten indices for batch processing\n",
    "        idx = idx.reshape(-1)  # Flatten completely\n",
    "        gathered_xyz = xyz.reshape(batch_size * n, -1)[idx, :]  # Gather points\n",
    "        gathered_xyz = gathered_xyz.reshape(batch_size, npoint, nsample, -1)  # Reshape back\n",
    "\n",
    "    return gathered_xyz\n",
    "\n",
    "\n",
    "def query_and_group(xyz, new_xyz, points, radius, nsample):\n",
    "    B, N, _ = xyz.shape\n",
    "    _, npoint, _ = new_xyz.shape\n",
    "\n",
    "    # Compute squared distances between sampled points and all points\n",
    "    sqrdists = square_distance(new_xyz, xyz)  # [B, npoint, N]\n",
    "\n",
    "    # Find indices of the nearest neighbors\n",
    "    group_idx = sqrdists.argsort(dim=-1)[:, :, :nsample]  # [B, npoint, nsample]\n",
    "\n",
    "    # Gather the grouped xyz coordinates\n",
    "    grouped_xyz = gather_points(xyz, group_idx)  # [B, npoint, nsample, 3]\n",
    "    grouped_xyz = grouped_xyz - new_xyz.unsqueeze(2)  # Local coordinates [B, npoint, nsample, 3]\n",
    "\n",
    "    if points is not None:\n",
    "        grouped_points = gather_points(points.transpose(1, 2), group_idx).permute(0, 3, 2, 1)  # [B, C, nsample, npoint]\n",
    "        new_points = torch.cat([grouped_xyz.permute(0, 3, 2, 1), grouped_points], dim=1)  # [B, C+3, nsample, npoint]\n",
    "    else:\n",
    "        new_points = grouped_xyz.permute(0, 3, 2, 1)  # [B, 3, nsample, npoint]\n",
    "\n",
    "    return new_points\n",
    "\n",
    "\n",
    "def square_distance(src, dst):\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))  # [B, N, M]\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist\n",
    "\n",
    "\n",
    "class SetAbstraction(nn.Module):\n",
    "    def __init__(self, npoint, radius, nsample, in_channel, mlp):\n",
    "        super(SetAbstraction, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius = radius\n",
    "        self.nsample = nsample\n",
    "\n",
    "        # Include x, y, z coordinates in the input channel count\n",
    "        last_channel = in_channel + 3  # Add (x, y, z)\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))  # Conv2D expects [B, C_in, nsample, npoint]\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        if self.npoint is not None:\n",
    "            idx = farthest_point_sample(xyz, self.npoint)  # [B, npoint]\n",
    "            new_xyz = gather_points(xyz, idx)  # [B, npoint, 3]\n",
    "        else:\n",
    "            new_xyz = xyz  # Use all points if npoint is None\n",
    "\n",
    "        grouped_points = query_and_group(xyz, new_xyz, points, self.radius, self.nsample)  # [B, C+3, nsample, npoint]\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            grouped_points = F.relu(self.mlp_bns[i](conv(grouped_points)))  # [B, out_channel, nsample, npoint]\n",
    "\n",
    "        # Max pooling over nsample dimension\n",
    "        new_points = torch.max(grouped_points, 2)[0]  # [B, mlp[-1], npoint]\n",
    "        return new_xyz, new_points\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Lớp self-attention cho đặc trưng điểm trong point cloud.\n",
    "    Dùng sau mỗi tầng Set Abstraction để tăng hiệu suất mô hình học hình học.\n",
    "    Input: [B, C, N] (batch, channel, num_points)\n",
    "    Output: [B, C, N]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, heads=4):\n",
    "        super(Attention, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.heads = heads\n",
    "        self.dk = in_channels // heads\n",
    "        assert in_channels % heads == 0, \"in_channels phải chia hết cho số heads\"\n",
    "        self.query = nn.Conv1d(in_channels, in_channels, 1)\n",
    "        self.key = nn.Conv1d(in_channels, in_channels, 1)\n",
    "        self.value = nn.Conv1d(in_channels, in_channels, 1)\n",
    "        self.proj = nn.Conv1d(in_channels, in_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, N]\n",
    "        B, C, N = x.shape\n",
    "        Q = self.query(x).view(B, self.heads, self.dk, N)  # [B, heads, dk, N]\n",
    "        K = self.key(x).view(B, self.heads, self.dk, N)\n",
    "        V = self.value(x).view(B, self.heads, self.dk, N)\n",
    "        attn = torch.einsum('bhdk,bhdk->bhdn', Q, K) / (self.dk ** 0.5)  # [B, heads, N, N]\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        out = torch.einsum('bhdn,bhdn->bhdk', attn, V)  # [B, heads, dk, N]\n",
    "        out = out.contiguous().view(B, C, N)\n",
    "        out = self.proj(out)\n",
    "        return out + x  # residual\n",
    "    \n",
    "\n",
    "class GLUBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear_main = nn.Linear(in_dim, out_dim)\n",
    "        self.linear_gate = nn.Linear(in_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_main(x) * torch.sigmoid(self.linear_gate(x))\n",
    "\n",
    "# Define PointNet++ model\n",
    "class PointNetPlusPlus(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PointNetPlusPlus, self).__init__()\n",
    "\n",
    "        # Set Abstraction layers\n",
    "        self.sa1 = SetAbstraction(npoint=512, radius=0.2, nsample=32, in_channel=0, mlp=[64,128])\n",
    "        self.sa1_attention = Attention(in_channels=128, heads=4)  # Attention after first SA layer\n",
    "        self.sa2 = SetAbstraction(npoint=128, radius=0.4, nsample=64, in_channel=128, mlp=[128, 256])\n",
    "        self.sa2_attention = Attention(in_channels=256, heads=4)  # Attention after second SA layer\n",
    "        self.sa3 = SetAbstraction(npoint=None, radius=None, nsample=None, in_channel=256, mlp=[256,512, 1024])\n",
    "        self.sa3_attention = Attention(in_channels=1024, heads=4)  # Attention after third SA layer\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        self.light_head = LightHead(in_dim=1024, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, xyz):\n",
    "        batch_size, _, _ = xyz.shape\n",
    "\n",
    "        # Hierarchical feature extraction\n",
    "        l1_xyz, l1_points = self.sa1(xyz, None)       # Layer 1: [B, 512, 128]\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)  # Layer 2: [B, 128, 256]\n",
    "        _, l3_points = self.sa3(l2_xyz, l2_points)    # Layer 3: [B, 1024, npoint]\n",
    "        # Fully connected layers\n",
    "        x = self.light_head(l3_points)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee1d15",
   "metadata": {},
   "source": [
    "<h2> Dataset utils </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dfe2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_labels(label_dir):\n",
    "    unique_labels = set()\n",
    "    for label_file in os.listdir(label_dir):\n",
    "        if label_file.endswith('.txt'):\n",
    "            with open(os.path.join(label_dir, label_file), 'r') as file:\n",
    "                for line in file:\n",
    "                    parts = line.strip().split()\n",
    "                    unique_labels.add(parts[0])  # Add the label (Class)\n",
    "    return sorted(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49437b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_to_pcd(binFileName):\n",
    "    size_float = 4\n",
    "    list_pcd = []\n",
    "    with open(binFileName, \"rb\") as f:\n",
    "        byte = f.read(size_float * 4)\n",
    "        while byte:\n",
    "            x, y, z, intensity = struct.unpack(\"ffff\", byte)\n",
    "            list_pcd.append([x, y, z])\n",
    "            byte = f.read(size_float * 4)\n",
    "    np_pcd = np.asarray(list_pcd)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(np_pcd)\n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37f766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted label mapping: {'Car': 0, 'Cyclist': 1, 'DontCare': 2, 'Misc': 3, 'Pedestrian': 4, 'Person_sitting': 5, 'Tram': 6, 'Truck': 7, 'Van': 8}\n"
     ]
    }
   ],
   "source": [
    "velodyne_dir = r\"D:\\Workspace\\Python\\Point_Cloud_3D_Object_Detection\\data\\training\\velodyne\"\n",
    "label_dir = r\"D:\\Workspace\\Python\\Point_Cloud_3D_Object_Detection\\data\\training\\label_2\"\n",
    "calib_dir = r\"D:\\Workspace\\Python\\Point_Cloud_3D_Object_Detection\\data\\training\\calib\"\n",
    "unique_labels = extract_unique_labels(label_dir)\n",
    "label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "print(\"Extracted label mapping:\", label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb2fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_velodyne_bin(bin_path):\n",
    "    \"\"\"\n",
    "    Đọc file .bin từ KITTI và trả về mảng (N, 4): x, y, z, reflectance\n",
    "    \"\"\"\n",
    "    return np.fromfile(bin_path, dtype=np.float32).reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe756d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kitti_label(label_file):\n",
    "    \"\"\"\n",
    "    Đọc file .txt nhãn từ KITTI object detection.\n",
    "    Trả về danh sách bounding box + class:\n",
    "    [class, x, y, z, h, w, l, ry]\n",
    "    \"\"\"\n",
    "    if not label_file.endswith('.txt'):\n",
    "        return None\n",
    "    boxes = []\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            parts = line.strip().split(' ')\n",
    "            cls = parts[0]\n",
    "            if cls == 'DontCare':\n",
    "                continue\n",
    "            # Extract 3D box info\n",
    "            h, w, l = map(float, parts[8:11])\n",
    "            x, y, z = map(float, parts[11:14])\n",
    "            ry = float(parts[14])\n",
    "            boxes.append({\n",
    "                'class': cls,\n",
    "                'center': [x, y, z],\n",
    "                'size': [l, w, h],\n",
    "                'rotation': ry\n",
    "            })\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d25678ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_calib_file(calib_path):\n",
    "    \"\"\"\n",
    "    Đọc file calibration của KITTI và trả về các ma trận chuyển đổi\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    with open(calib_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                data[key] = np.array([float(x) for x in value.strip().split()])\n",
    "    \n",
    "    # Chuyển về ma trận đúng shape\n",
    "    data['Tr_velo_to_cam'] = data['Tr_velo_to_cam'].reshape(3, 4)\n",
    "    data['R0_rect'] = data['R0_rect'].reshape(3, 3)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f8d7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam_to_velo(xyz_cam, calib):\n",
    "    \"\"\"\n",
    "    Chuyển đổi tọa độ từ camera sang lidar (velodyne)\n",
    "    xyz_cam: (N, 3) - tọa độ trong hệ camera\n",
    "    calib: dict chứa các ma trận calibration\n",
    "    \"\"\"\n",
    "    # Thêm 1 vào cuối để thành (N, 4) - homogeneous coordinates\n",
    "    xyz_cam_hom = np.hstack([xyz_cam, np.ones((xyz_cam.shape[0], 1))])\n",
    "    \n",
    "    # Lấy ma trận chuyển đổi từ velodyne sang camera\n",
    "    Tr = calib['Tr_velo_to_cam']  # (3, 4)\n",
    "    \n",
    "    # Tính ma trận nghịch đảo để chuyển từ camera sang velodyne\n",
    "    Tr_inv = np.linalg.pinv(np.vstack([Tr, [0,0,0,1]]))  # (4,4)\n",
    "    \n",
    "    # Chuyển đổi tọa độ\n",
    "    xyz_velo = (Tr_inv @ xyz_cam_hom.T).T[:, :3]\n",
    "    return xyz_velo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5af69846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_3d_box_to_velo(box, calib):\n",
    "    \"\"\"\n",
    "    Chuyển đổi 3D bounding box từ hệ camera sang hệ lidar\n",
    "    box: dict chứa thông tin box {'center': [x,y,z], 'size': [l,w,h], 'rotation': ry}\n",
    "    calib: dict chứa các ma trận calibration\n",
    "    \"\"\"\n",
    "    center_cam = np.array([box['center']])  # (1, 3)\n",
    "    center_velo = cam_to_velo(center_cam, calib)[0]  # (3,)\n",
    "    \n",
    "    # Kích thước box không đổi khi chuyển hệ tọa độ\n",
    "    size_velo = box['size']  # [l, w, h]\n",
    "    \n",
    "    # Góc quay cần điều chỉnh (thường không đổi nhiều)\n",
    "    rotation_velo = box['rotation']\n",
    "    \n",
    "    return {\n",
    "        'center': center_velo.tolist(),\n",
    "        'size': size_velo,\n",
    "        'rotation': rotation_velo,\n",
    "        'class': box['class']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90172b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_objects_from_pointcloud_with_calib(points, bboxes, class_map, calib):\n",
    "    \"\"\"\n",
    "    Trích xuất object từ point cloud với chuyển đổi hệ tọa độ\n",
    "    \"\"\"\n",
    "    objects = []\n",
    "    for box in bboxes:\n",
    "        cls = box['class']\n",
    "        if cls not in class_map:\n",
    "            continue\n",
    "            \n",
    "        # Chuyển đổi box từ camera sang lidar\n",
    "        box_velo = convert_3d_box_to_velo(box, calib)\n",
    "        \n",
    "        center, size = box_velo['center'], box_velo['size']\n",
    "        l, w, h = size\n",
    "        x, y, z = center\n",
    "\n",
    "        # Hộp trục song song (AABB) trong hệ lidar\n",
    "        mask = (\n",
    "            (points[:, 0] > x - l/2) & (points[:, 0] < x + l/2) &\n",
    "            (points[:, 1] > y - w/2) & (points[:, 1] < y + w/2) &\n",
    "            (points[:, 2] > z - h/2) & (points[:, 2] < z + h/2)\n",
    "        )\n",
    "        pc_object = points[mask][:, :3]\n",
    "        \n",
    "        if len(pc_object) >= 30:  # Chỉ lấy object có đủ điểm\n",
    "            label_id = class_map[str(cls)]\n",
    "            objects.append((pc_object, label_id))\n",
    "            \n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28c6cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_objects_with_calib(velodyne_dir, label_dir, calib_dir, class_map):\n",
    "    \"\"\"\n",
    "    Trích xuất tất cả object từ point cloud với chuyển đổi hệ tọa độ\n",
    "    \"\"\"\n",
    "    all_objects = []  # List chứa (pc_object, class_id)\n",
    "    bin_files = sorted(glob.glob(os.path.join(velodyne_dir, \"*.bin\")))\n",
    "\n",
    "    for bin_path in bin_files:\n",
    "        file_id = os.path.splitext(os.path.basename(bin_path))[0]  # '000012'\n",
    "\n",
    "        # Đường dẫn đến file label và calib tương ứng\n",
    "        label_path = os.path.join(label_dir, f\"{file_id}.txt\")\n",
    "        calib_path = os.path.join(calib_dir, f\"{file_id}.txt\")\n",
    "        \n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"[!] Thiếu label cho {file_id}, bỏ qua\")\n",
    "            continue\n",
    "            \n",
    "        if not os.path.exists(calib_path):\n",
    "            print(f\"[!] Thiếu calib cho {file_id}, bỏ qua\")\n",
    "            continue\n",
    "\n",
    "        # Đọc dữ liệu\n",
    "        points = read_velodyne_bin(bin_path)\n",
    "        bboxes = read_kitti_label(label_path)\n",
    "        calib = read_calib_file(calib_path)\n",
    "        \n",
    "        # Trích xuất object với chuyển đổi hệ tọa độ\n",
    "        objects = extract_objects_from_pointcloud_with_calib(points, bboxes, class_map, calib)\n",
    "        all_objects.extend(objects)\n",
    "        \n",
    "    return all_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cc68b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point cloud shape: (115384, 4)\n",
      "Số bounding boxes: 1\n",
      "Calib keys: ['P0', 'P1', 'P2', 'P3', 'R0_rect', 'Tr_velo_to_cam', 'Tr_imu_to_velo']\n",
      "\n",
      "Box gốc (camera): {'class': 'Pedestrian', 'center': [1.84, 1.47, 8.41], 'size': [1.2, 0.48, 1.89], 'rotation': 0.01}\n",
      "Box sau chuyển đổi (lidar): {'center': [8.753024291915839, -1.7997219565910108, -1.5464078787587405], 'size': [1.2, 0.48, 1.89], 'rotation': 0.01, 'class': 'Pedestrian'}\n",
      "Số object trích xuất được: 1\n",
      "Object 0: class_id=1, num_points=194\n"
     ]
    }
   ],
   "source": [
    "# Hàm test để kiểm tra chuyển đổi\n",
    "def test_coordinate_conversion():\n",
    "    \"\"\"\n",
    "    Hàm test để kiểm tra việc chuyển đổi hệ tọa độ\n",
    "    \"\"\"\n",
    "    # Đường dẫn test\n",
    "    velodyne_dir = r\"E:\\Storange\\Python\\Point_cloud\\data\\archive\\training\\velodyne_subset\"\n",
    "    label_dir = r\"E:\\Storange\\Python\\Point_cloud\\data\\archive\\training\\label_2_subset\"\n",
    "    calib_dir = r\"E:\\Storange\\Python\\Point_cloud\\data\\archive\\training\\calib_subset\"\n",
    "    \n",
    "    # Test với file đầu tiên\n",
    "    test_file_id = \"000000\"\n",
    "    \n",
    "    # Đọc dữ liệu\n",
    "    points = read_velodyne_bin(os.path.join(velodyne_dir, f\"{test_file_id}.bin\"))\n",
    "    bboxes = read_kitti_label(os.path.join(label_dir, f\"{test_file_id}.txt\"))\n",
    "    calib = read_calib_file(os.path.join(calib_dir, f\"{test_file_id}.txt\"))\n",
    "    \n",
    "    print(f\"Point cloud shape: {points.shape}\")\n",
    "    print(f\"Số bounding boxes: {len(bboxes)}\")\n",
    "    print(f\"Calib keys: {list(calib.keys())}\")\n",
    "    \n",
    "    # Test chuyển đổi một box\n",
    "    if len(bboxes) > 0:\n",
    "        box = bboxes[0]\n",
    "        print(f\"\\nBox gốc (camera): {box}\")\n",
    "        \n",
    "        box_velo = convert_3d_box_to_velo(box, calib)\n",
    "        print(f\"Box sau chuyển đổi (lidar): {box_velo}\")\n",
    "        \n",
    "        # Test trích xuất object\n",
    "        class_map = {'Car': 0, 'Pedestrian': 1, 'Cyclist': 2}\n",
    "        objects = extract_objects_from_pointcloud_with_calib(points, bboxes, class_map, calib)\n",
    "        print(f\"Số object trích xuất được: {len(objects)}\")\n",
    "        \n",
    "        for i, (pc_obj, label_id) in enumerate(objects):\n",
    "            print(f\"Object {i}: class_id={label_id}, num_points={len(pc_obj)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_coordinate_conversion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff24ba94",
   "metadata": {},
   "source": [
    "<h2> Class Dataset </h2>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19a1f652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số object trích ra: 3263\n"
     ]
    }
   ],
   "source": [
    "all_objs = extract_all_objects_with_calib(velodyne_dir, label_dir,  calib_dir,label_to_id)\n",
    "\n",
    "print(f\"Tổng số object trích ra: {len(all_objs)}\")\n",
    "# all_objs = [(pc1, label1), (pc2, label2), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38a1e770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 3)\n"
     ]
    }
   ],
   "source": [
    "for pc_obj, label in all_objs:\n",
    "    print(pc_obj.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed9926d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KittiObjectDataset(Dataset):\n",
    "    def __init__(self,all_objs,num_points= 1024) -> None:\n",
    "        self.all_objs = all_objs\n",
    "\n",
    "        self.num_points = num_points\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.all_objs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        pc_obj, label = self.all_objs[index]\n",
    "\n",
    "        if len(pc_obj) > self.num_points:\n",
    "            idxs = np.random.choice(len(pc_obj), self.num_points, replace=False)\n",
    "        else:\n",
    "            idxs = np.random.choice(len(pc_obj), self.num_points, replace=True)\n",
    "        pc_obj = pc_obj[idxs]\n",
    "\n",
    "        return torch.tensor(pc_obj, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be00374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "from torch import optim\n",
    "NUM_POINTS= 1024\n",
    "NUM_CLASSES = len(label_to_id)\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PointNetPlusPlus(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0359c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "total_params = 0\n",
    "for name, parameter in model.named_parameters():\n",
    "    if not parameter.requires_grad: continue\n",
    "    params = parameter.numel()\n",
    "    table.add_row([name, params])\n",
    "    total_params+=params\n",
    "print(table)\n",
    "print(f\"Total Trainable Params: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset = KittiObjectDataset(all_objs, num_points=NUM_POINTS)\n",
    "\n",
    "len_dataset = len(dataset)\n",
    "print(f\"Tổng số mẫu trong dataset: {len_dataset}\")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset,\n",
    "                                          [round(0.7*len_dataset), round(0.1*len_dataset), round(0.2*len_dataset)],\n",
    "                                          generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ddf3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    points, labels = batch\n",
    "    print(points.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c22859",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "best_loss= np.inf\n",
    "\n",
    "def training_loop(epochs, model, train_loader,val_dataloader, optimizer, criterion,num_points):\n",
    "    \"\"\"\n",
    "    Hàm huấn luyện mô hình PointNet++ với dữ liệu point cloud từ KITTI.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        epoch_train_loss = []\n",
    "        epoch_train_acc = []\n",
    "\n",
    "        for points, labels in train_loader:\n",
    "            points, labels = points.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(points)  # No need for reshaping\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            epoch_train_loss.append(loss.cpu().item())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            total += labels.size(0)\n",
    "\n",
    "            accuracy = float(total.item() / correct)\n",
    "            epoch_train_acc.append(accuracy)\n",
    "\n",
    "        epoch_test_loss = []\n",
    "        epoch_test_acc = []\n",
    "\n",
    "        for points, labels in val_dataloader:\n",
    "            points, labels = points.to(device), labels.to(device)\n",
    "\n",
    "            model = model.eval()\n",
    "            outputs = model(points)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_test_loss.append(loss.cpu().item())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            accuracy = float(correct / labels.size(0))\n",
    "            epoch_test_acc.append(accuracy)\n",
    "    print('Epoch %s: train loss: %s, val loss: %f, train accuracy: %s,  val accuracy: %f'\n",
    "              % (epoch,\n",
    "                round(np.mean(epoch_train_loss), 4),\n",
    "                round(np.mean(epoch_test_loss), 4),\n",
    "                round(np.mean(epoch_train_acc), 4),\n",
    "                round(np.mean(epoch_test_acc), 4)))\n",
    "    if np.mean(test_loss) < best_loss:\n",
    "        state = {\n",
    "            'model':model.state_dict(),\n",
    "            'optimizer':optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(state, os.path.join('checkpoints', '3DKitti_checkpoint_%s.pth' % (num_points)))\n",
    "        best_loss=np.mean(test_loss)\n",
    "    train_loss.append(np.mean(epoch_train_loss))\n",
    "    test_loss.append(np.mean(epoch_test_loss))\n",
    "    train_acc.append(np.mean(epoch_train_acc))\n",
    "    test_acc.append(np.mean(epoch_test_acc))\n",
    "    return train_loss, train_acc,test_loss,test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f156384",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc,test_loss,test_acc= training_loop(EPOCHS, model, train_dataloader,val_dataloader, optimizer, criterion,NUM_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd6d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_loss, test_loss, save_to_file=None):\n",
    "    fig = plt.figure()\n",
    "    epochs = len(train_loss)\n",
    "    plt.plot(range(epochs), train_loss, 'b', label='Training loss')\n",
    "    plt.plot(range(epochs), test_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    if save_to_file:\n",
    "        fig.savefig(save_to_file,dpi=200)\n",
    "\n",
    "def plot_accuracy(train_acc, test_acc, save_to_file=None):\n",
    "    fig = plt.figure()\n",
    "    epochs = len(train_acc)\n",
    "    plt.plot(range(epochs), train_acc, 'b', label='Training accuracy')\n",
    "    plt.plot(range(epochs), test_acc, 'r', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    if save_to_file:\n",
    "        fig.savefig(save_to_file,dpi=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_loss, test_loss, save_to_file=False)\n",
    "plot_accuracy(train_acc, test_acc, save_to_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79107082",
   "metadata": {},
   "source": [
    "<h2> Testing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure model is in evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "# # Initialize a list to store predictions\n",
    "# predictions = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for points in test_loader:\n",
    "#         points = points.to(device)  # Send points to GPU if available\n",
    "#         outputs = model(points)  # Get predictions\n",
    "#         _, predicted_classes = torch.max(outputs, 1)  # Predicted class indices\n",
    "#         predictions.append(predicted_classes.cpu().numpy())  # Store predictions\n",
    "\n",
    "# # Flatten predictions into a single array\n",
    "# predictions = np.concatenate(predictions, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
