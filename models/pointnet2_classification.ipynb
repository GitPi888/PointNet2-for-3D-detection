{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a6be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import collections\n",
    "from scipy.linalg import expm,norm\n",
    "import glob\n",
    "import open3d as o3d\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76f3f1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: PHUC\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "print(\"Running on:\", socket.gethostname())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92fcbb6",
   "metadata": {},
   "source": [
    "<h2> Build PointNet++ </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "904e55e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions:\n",
    "def farthest_point_sample(xyz, npoint):\n",
    "    batch_size, n, _ = xyz.shape\n",
    "    device = xyz.device\n",
    "    centroids = torch.zeros(batch_size, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(batch_size, n).to(device) * 1e10\n",
    "    farthest = torch.randint(0, n, (batch_size,), dtype=torch.long).to(device)\n",
    "    batch_indices = torch.arange(batch_size, dtype=torch.long).to(device)\n",
    "\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].unsqueeze(1)  # [B, 1, 3]\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1)  # Squared distance [B, N]\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = torch.max(distance, -1)[1]  # Index of the farthest point\n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def gather_points(xyz, idx):\n",
    "    batch_size, n, _ = xyz.shape\n",
    "\n",
    "    if idx.dim() == 2:  # Case 1: [B, npoint]\n",
    "        _, npoint = idx.shape\n",
    "        idx = idx.view(-1)  # Flatten indices for batch processing\n",
    "        gathered_xyz = xyz.reshape(batch_size * n, -1)[idx, :]  # Gather points\n",
    "        gathered_xyz = gathered_xyz.reshape(batch_size, npoint, -1)  # Reshape back\n",
    "    elif idx.dim() == 3:  # Case 2: [B, npoint, nsample]\n",
    "        _, npoint, nsample = idx.shape\n",
    "        idx_base = torch.arange(0, batch_size, device=xyz.device).view(-1, 1, 1) * n\n",
    "        idx = idx + idx_base  # Flatten indices for batch processing\n",
    "        idx = idx.reshape(-1)  # Flatten completely\n",
    "        gathered_xyz = xyz.reshape(batch_size * n, -1)[idx, :]  # Gather points\n",
    "        gathered_xyz = gathered_xyz.reshape(batch_size, npoint, nsample, -1)  # Reshape back\n",
    "\n",
    "    return gathered_xyz\n",
    "\n",
    "\n",
    "def query_and_group(xyz, new_xyz, points, radius, nsample):\n",
    "    B, N, _ = xyz.shape\n",
    "    _, npoint, _ = new_xyz.shape\n",
    "\n",
    "    # Compute squared distances between sampled points and all points\n",
    "    sqrdists = square_distance(new_xyz, xyz)  # [B, npoint, N]\n",
    "\n",
    "    # Find indices of the nearest neighbors\n",
    "    group_idx = sqrdists.argsort(dim=-1)[:, :, :nsample]  # [B, npoint, nsample]\n",
    "\n",
    "    # Gather the grouped xyz coordinates\n",
    "    grouped_xyz = gather_points(xyz, group_idx)  # [B, npoint, nsample, 3]\n",
    "    grouped_xyz = grouped_xyz - new_xyz.unsqueeze(2)  # Local coordinates [B, npoint, nsample, 3]\n",
    "\n",
    "    if points is not None:\n",
    "        grouped_points = gather_points(points.transpose(1, 2), group_idx).permute(0, 3, 2, 1)  # [B, C, nsample, npoint]\n",
    "        new_points = torch.cat([grouped_xyz.permute(0, 3, 2, 1), grouped_points], dim=1)  # [B, C+3, nsample, npoint]\n",
    "    else:\n",
    "        new_points = grouped_xyz.permute(0, 3, 2, 1)  # [B, 3, nsample, npoint]\n",
    "\n",
    "    return new_points\n",
    "\n",
    "\n",
    "def square_distance(src, dst):\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))  # [B, N, M]\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist\n",
    "\n",
    "\n",
    "class SetAbstraction(nn.Module):\n",
    "    def __init__(self, npoint, radius, nsample, in_channel, mlp):\n",
    "        super(SetAbstraction, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius = radius\n",
    "        self.nsample = nsample\n",
    "\n",
    "        # Include x, y, z coordinates in the input channel count\n",
    "        last_channel = in_channel + 3  # Add (x, y, z)\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))  # Conv2D expects [B, C_in, nsample, npoint]\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        if self.npoint is not None:\n",
    "            idx = farthest_point_sample(xyz, self.npoint)  # [B, npoint]\n",
    "            new_xyz = gather_points(xyz, idx)  # [B, npoint, 3]\n",
    "        else:\n",
    "            new_xyz = xyz  # Use all points if npoint is None\n",
    "\n",
    "        grouped_points = query_and_group(xyz, new_xyz, points, self.radius, self.nsample)  # [B, C+3, nsample, npoint]\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            grouped_points = F.relu(self.mlp_bns[i](conv(grouped_points)))  # [B, out_channel, nsample, npoint]\n",
    "\n",
    "        # Max pooling over nsample dimension\n",
    "        new_points = torch.max(grouped_points, 2)[0]  # [B, mlp[-1], npoint]\n",
    "        return new_xyz, new_points\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Lớp self-attention cho đặc trưng điểm trong point cloud.\n",
    "    Dùng sau mỗi tầng Set Abstraction để tăng hiệu suất mô hình học hình học.\n",
    "    Input: [B, C, N] (batch, channel, num_points)\n",
    "    Output: [B, C, N]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, heads=4):\n",
    "        super(Attention, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.heads = heads\n",
    "        self.dk = in_channels // heads\n",
    "        assert in_channels % heads == 0, \"in_channels phải chia hết cho số heads\"\n",
    "        self.query = nn.Conv1d(in_channels, in_channels, 1)\n",
    "        self.key = nn.Conv1d(in_channels, in_channels, 1)\n",
    "        self.value = nn.Conv1d(in_channels, in_channels, 1)\n",
    "        self.proj = nn.Conv1d(in_channels, in_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, N]\n",
    "        B, C, N = x.shape\n",
    "        Q = self.query(x).view(B, self.heads, self.dk, N)  # [B, heads, dk, N]\n",
    "        K = self.key(x).view(B, self.heads, self.dk, N)\n",
    "        V = self.value(x).view(B, self.heads, self.dk, N)\n",
    "        attn = torch.einsum('bhdk,bhdk->bhdn', Q, K) / (self.dk ** 0.5)  # [B, heads, N, N]\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        out = torch.einsum('bhdn,bhdn->bhdk', attn, V)  # [B, heads, dk, N]\n",
    "        out = out.contiguous().view(B, C, N)\n",
    "        out = self.proj(out)\n",
    "        return out + x  # residual\n",
    "    \n",
    "\n",
    "class GLUBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear_main = nn.Linear(in_dim, out_dim)\n",
    "        self.linear_gate = nn.Linear(in_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_main(x) * torch.sigmoid(self.linear_gate(x))\n",
    "\n",
    "class LightHead(nn.Module):\n",
    "    def __init__(self, in_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.conv1 = nn.Conv1d(in_dim, 128, 1)\n",
    "        self.conv2 = nn.Conv1d(128, num_classes, 1)\n",
    "        \n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.bn2 = nn.BatchNorm1d(num_classes)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.global_pool(x)\n",
    "        x = F.relu(self.dropout1(self.bn1(self.conv1(x)))) \n",
    "        x = F.relu(self.dropout2(self.bn2(self.conv2(x))))\n",
    "        x= x.squeeze(dim=-1)\n",
    "        return x\n",
    "# Define PointNet++ model\n",
    "class PointNetPlusPlus(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PointNetPlusPlus, self).__init__()\n",
    "\n",
    "        # Set Abstraction layers\n",
    "        self.sa1 = SetAbstraction(npoint=512, radius=0.2, nsample=32, in_channel=0, mlp=[64,128])\n",
    "        self.sa1_attention = Attention(in_channels=128, heads=4)  # Attention after first SA layer\n",
    "        self.sa2 = SetAbstraction(npoint=128, radius=0.4, nsample=64, in_channel=128, mlp=[128, 256])\n",
    "        self.sa2_attention = Attention(in_channels=256, heads=4)  # Attention after second SA layer\n",
    "        self.sa3 = SetAbstraction(npoint=None, radius=None, nsample=None, in_channel=256, mlp=[256,512, 1024])\n",
    "        self.sa3_attention = Attention(in_channels=1024, heads=4)  # Attention after third SA layer\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        self.light_head = LightHead(in_dim=1024, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, xyz):\n",
    "        batch_size, _, _ = xyz.shape\n",
    "\n",
    "        # Hierarchical feature extraction\n",
    "        l1_xyz, l1_points = self.sa1(xyz, None)       # Layer 1: [B, 512, 128]\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)  # Layer 2: [B, 128, 256]\n",
    "        _, l3_points = self.sa3(l2_xyz, l2_points)    # Layer 3: [B, 1024, npoint]\n",
    "        # Fully connected layers\n",
    "        x = self.light_head(l3_points)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee1d15",
   "metadata": {},
   "source": [
    "<h2> Dataset utils </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dfe2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_labels(label_dir):\n",
    "    unique_labels = set()\n",
    "    for label_file in os.listdir(label_dir):\n",
    "        if label_file.endswith('.txt'):\n",
    "            with open(os.path.join(label_dir, label_file), 'r') as file:\n",
    "                for line in file:\n",
    "                    parts = line.strip().split()\n",
    "                    unique_labels.add(parts[0])  # Add the label (Class)\n",
    "    return sorted(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e49437b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_to_pcd(binFileName):\n",
    "    size_float = 4\n",
    "    list_pcd = []\n",
    "    with open(binFileName, \"rb\") as f:\n",
    "        byte = f.read(size_float * 4)\n",
    "        while byte:\n",
    "            x, y, z, intensity = struct.unpack(\"ffff\", byte)\n",
    "            list_pcd.append([x, y, z])\n",
    "            byte = f.read(size_float * 4)\n",
    "    np_pcd = np.asarray(list_pcd)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(np_pcd)\n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec37f766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted label mapping: {'Car': 0, 'Cyclist': 1, 'DontCare': 2, 'Misc': 3, 'Pedestrian': 4, 'Person_sitting': 5, 'Tram': 6, 'Truck': 7, 'Van': 8}\n"
     ]
    }
   ],
   "source": [
    "velodyne_dir = r\"D:\\Workspace\\Python\\Data\\training\\velodyne\"\n",
    "label_dir = r\"D:\\Workspace\\Python\\Data\\training\\label_2\"\n",
    "calib_dir = r\"D:\\Workspace\\Python\\Data\\training\\calib\"\n",
    "unique_labels = extract_unique_labels(label_dir)\n",
    "label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "print(\"Extracted label mapping:\", label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afb2fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_velodyne_bin(bin_path):\n",
    "    \"\"\"\n",
    "    Đọc file .bin từ KITTI và trả về mảng (N, 4): x, y, z, reflectance\n",
    "    \"\"\"\n",
    "    return np.fromfile(bin_path, dtype=np.float32).reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfe756d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kitti_label(label_file):\n",
    "    \"\"\"\n",
    "    Đọc file .txt nhãn từ KITTI object detection.\n",
    "    Trả về danh sách bounding box + class:\n",
    "    [class, x, y, z, h, w, l, ry]\n",
    "    \"\"\"\n",
    "    if not label_file.endswith('.txt'):\n",
    "        return None\n",
    "    boxes = []\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            parts = line.strip().split(' ')\n",
    "            cls = parts[0]\n",
    "            if cls == 'DontCare':\n",
    "                continue\n",
    "            # Extract 3D box info\n",
    "            h, w, l = map(float, parts[8:11])\n",
    "            x, y, z = map(float, parts[11:14])\n",
    "            ry = float(parts[14])\n",
    "            boxes.append({\n",
    "                'class': cls,\n",
    "                'center': [x, y, z],\n",
    "                'size': [l, w, h],\n",
    "                'rotation': ry\n",
    "            })\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d25678ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_calib_file(calib_path):\n",
    "    \"\"\"\n",
    "    Đọc file calibration của KITTI và trả về các ma trận chuyển đổi\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    with open(calib_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                data[key] = np.array([float(x) for x in value.strip().split()])\n",
    "    \n",
    "    # Chuyển về ma trận đúng shape\n",
    "    data['Tr_velo_to_cam'] = data['Tr_velo_to_cam'].reshape(3, 4)\n",
    "    data['R0_rect'] = data['R0_rect'].reshape(3, 3)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f8d7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam_to_velo(xyz_cam, calib):\n",
    "    \"\"\"\n",
    "    Chuyển đổi tọa độ từ camera sang lidar (velodyne)\n",
    "    xyz_cam: (N, 3) - tọa độ trong hệ camera\n",
    "    calib: dict chứa các ma trận calibration\n",
    "    \"\"\"\n",
    "    # Thêm 1 vào cuối để thành (N, 4) - homogeneous coordinates\n",
    "    xyz_cam_hom = np.hstack([xyz_cam, np.ones((xyz_cam.shape[0], 1))])\n",
    "    \n",
    "    # Lấy ma trận chuyển đổi từ velodyne sang camera\n",
    "    Tr = calib['Tr_velo_to_cam']  # (3, 4)\n",
    "    \n",
    "    # Tính ma trận nghịch đảo để chuyển từ camera sang velodyne\n",
    "    Tr_inv = np.linalg.pinv(np.vstack([Tr, [0,0,0,1]]))  # (4,4)\n",
    "    \n",
    "    # Chuyển đổi tọa độ\n",
    "    xyz_velo = (Tr_inv @ xyz_cam_hom.T).T[:, :3]\n",
    "    return xyz_velo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5af69846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_3d_box_to_velo(box, calib):\n",
    "    \"\"\"\n",
    "    Chuyển đổi 3D bounding box từ hệ camera sang hệ lidar\n",
    "    box: dict chứa thông tin box {'center': [x,y,z], 'size': [l,w,h], 'rotation': ry}\n",
    "    calib: dict chứa các ma trận calibration\n",
    "    \"\"\"\n",
    "    center_cam = np.array([box['center']])  # (1, 3)\n",
    "    center_velo = cam_to_velo(center_cam, calib)[0]  # (3,)\n",
    "    \n",
    "    # Kích thước box không đổi khi chuyển hệ tọa độ\n",
    "    size_velo = box['size']  # [l, w, h]\n",
    "    \n",
    "    # Góc quay cần điều chỉnh (thường không đổi nhiều)\n",
    "    rotation_velo = box['rotation']\n",
    "    \n",
    "    return {\n",
    "        'center': center_velo.tolist(),\n",
    "        'size': size_velo,\n",
    "        'rotation': rotation_velo,\n",
    "        'class': box['class']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90172b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_objects_from_pointcloud_with_calib(points, bboxes, class_map, calib):\n",
    "    \"\"\"\n",
    "    Trích xuất object từ point cloud với chuyển đổi hệ tọa độ\n",
    "    \"\"\"\n",
    "    objects = []\n",
    "    for box in bboxes:\n",
    "        cls = box['class']\n",
    "        if cls not in class_map:\n",
    "            continue\n",
    "            \n",
    "        # Chuyển đổi box từ camera sang lidar\n",
    "        box_velo = convert_3d_box_to_velo(box, calib)\n",
    "        \n",
    "        center, size = box_velo['center'], box_velo['size']\n",
    "        l, w, h = size\n",
    "        x, y, z = center\n",
    "\n",
    "        # Hộp trục song song (AABB) trong hệ lidar\n",
    "        mask = (\n",
    "            (points[:, 0] > x - l/2) & (points[:, 0] < x + l/2) &\n",
    "            (points[:, 1] > y - w/2) & (points[:, 1] < y + w/2) &\n",
    "            (points[:, 2] > z - h/2) & (points[:, 2] < z + h/2)\n",
    "        )\n",
    "        pc_object = points[mask][:, :3]\n",
    "        \n",
    "        if len(pc_object) >= 30:  # Chỉ lấy object có đủ điểm\n",
    "            label_id = class_map[str(cls)]\n",
    "            objects.append((pc_object, label_id))\n",
    "            \n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28c6cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_objects_with_calib(velodyne_dir, label_dir, calib_dir, class_map):\n",
    "    \"\"\"\n",
    "    Trích xuất tất cả object từ point cloud với chuyển đổi hệ tọa độ\n",
    "    \"\"\"\n",
    "    all_objects = []  # List chứa (pc_object, class_id)\n",
    "    bin_files = sorted(glob.glob(os.path.join(velodyne_dir, \"*.bin\")))\n",
    "\n",
    "    for bin_path in bin_files:\n",
    "        file_id = os.path.splitext(os.path.basename(bin_path))[0]  # '000012'\n",
    "\n",
    "        # Đường dẫn đến file label và calib tương ứng\n",
    "        label_path = os.path.join(label_dir, f\"{file_id}.txt\")\n",
    "        calib_path = os.path.join(calib_dir, f\"{file_id}.txt\")\n",
    "        \n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"[!] Thiếu label cho {file_id}, bỏ qua\")\n",
    "            continue\n",
    "            \n",
    "        if not os.path.exists(calib_path):\n",
    "            print(f\"[!] Thiếu calib cho {file_id}, bỏ qua\")\n",
    "            continue\n",
    "\n",
    "        # Đọc dữ liệu\n",
    "        points = read_velodyne_bin(bin_path)\n",
    "        bboxes = read_kitti_label(label_path)\n",
    "        calib = read_calib_file(calib_path)\n",
    "        \n",
    "        # Trích xuất object với chuyển đổi hệ tọa độ\n",
    "        objects = extract_objects_from_pointcloud_with_calib(points, bboxes, class_map, calib)\n",
    "        all_objects.extend(objects)\n",
    "        \n",
    "    return all_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc68b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm test để kiểm tra chuyển đổi\n",
    "def test_coordinate_conversion():\n",
    "    \"\"\"\n",
    "    Hàm test để kiểm tra việc chuyển đổi hệ tọa độ\n",
    "    \"\"\"\n",
    "    # Đường dẫn test\n",
    "    velodyne_dir = r\"E:\\Storange\\Python\\Point_cloud\\data\\archive\\training\\velodyne_subset\"\n",
    "    label_dir = r\"E:\\Storange\\Python\\Point_cloud\\data\\archive\\training\\label_2_subset\"\n",
    "    calib_dir = r\"E:\\Storange\\Python\\Point_cloud\\data\\archive\\training\\calib_subset\"\n",
    "    \n",
    "    # Test với file đầu tiên\n",
    "    test_file_id = \"000000\"\n",
    "    \n",
    "    # Đọc dữ liệu\n",
    "    points = read_velodyne_bin(os.path.join(velodyne_dir, f\"{test_file_id}.bin\"))\n",
    "    bboxes = read_kitti_label(os.path.join(label_dir, f\"{test_file_id}.txt\"))\n",
    "    calib = read_calib_file(os.path.join(calib_dir, f\"{test_file_id}.txt\"))\n",
    "    \n",
    "    print(f\"Point cloud shape: {points.shape}\")\n",
    "    print(f\"Số bounding boxes: {len(bboxes)}\")\n",
    "    print(f\"Calib keys: {list(calib.keys())}\")\n",
    "    \n",
    "    # Test chuyển đổi một box\n",
    "    if len(bboxes) > 0:\n",
    "        box = bboxes[0]\n",
    "        print(f\"\\nBox gốc (camera): {box}\")\n",
    "        \n",
    "        box_velo = convert_3d_box_to_velo(box, calib)\n",
    "        print(f\"Box sau chuyển đổi (lidar): {box_velo}\")\n",
    "        \n",
    "        # Test trích xuất object\n",
    "        class_map = {'Car': 0, 'Pedestrian': 1, 'Cyclist': 2}\n",
    "        objects = extract_objects_from_pointcloud_with_calib(points, bboxes, class_map, calib)\n",
    "        print(f\"Số object trích xuất được: {len(objects)}\")\n",
    "        \n",
    "        for i, (pc_obj, label_id) in enumerate(objects):\n",
    "            print(f\"Object {i}: class_id={label_id}, num_points={len(pc_obj)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_coordinate_conversion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff24ba94",
   "metadata": {},
   "source": [
    "<h2> Class Dataset </h2>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19a1f652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số object trích ra: 24709\n"
     ]
    }
   ],
   "source": [
    "all_objs = extract_all_objects_with_calib(velodyne_dir, label_dir,  calib_dir,label_to_id)\n",
    "\n",
    "print(f\"Tổng số object trích ra: {len(all_objs)}\")\n",
    "# all_objs = [(pc1, label1), (pc2, label2), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38a1e770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 3)\n"
     ]
    }
   ],
   "source": [
    "for pc_obj, label in all_objs:\n",
    "    print(pc_obj.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed9926d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KittiObjectDataset(Dataset):\n",
    "    def __init__(self,all_objs,num_points= 1024) -> None:\n",
    "        self.all_objs = all_objs\n",
    "\n",
    "        self.num_points = num_points\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.all_objs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        pc_obj, label = self.all_objs[index]\n",
    "\n",
    "        if len(pc_obj) > self.num_points:\n",
    "            idxs = np.random.choice(len(pc_obj), self.num_points, replace=False)\n",
    "        else:\n",
    "            idxs = np.random.choice(len(pc_obj), self.num_points, replace=True)\n",
    "        pc_obj = pc_obj[idxs]\n",
    "\n",
    "        return torch.tensor(pc_obj, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2be00374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "from torch import optim\n",
    "NUM_POINTS= 1024\n",
    "NUM_CLASSES = len(label_to_id)\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PointNetPlusPlus(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0359c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------+\n",
      "|          Modules           | Parameters |\n",
      "+----------------------------+------------+\n",
      "|   sa1.mlp_convs.0.weight   |    192     |\n",
      "|    sa1.mlp_convs.0.bias    |     64     |\n",
      "|   sa1.mlp_convs.1.weight   |    8192    |\n",
      "|    sa1.mlp_convs.1.bias    |    128     |\n",
      "|    sa1.mlp_bns.0.weight    |     64     |\n",
      "|     sa1.mlp_bns.0.bias     |     64     |\n",
      "|    sa1.mlp_bns.1.weight    |    128     |\n",
      "|     sa1.mlp_bns.1.bias     |    128     |\n",
      "| sa1_attention.query.weight |   16384    |\n",
      "|  sa1_attention.query.bias  |    128     |\n",
      "|  sa1_attention.key.weight  |   16384    |\n",
      "|   sa1_attention.key.bias   |    128     |\n",
      "| sa1_attention.value.weight |   16384    |\n",
      "|  sa1_attention.value.bias  |    128     |\n",
      "| sa1_attention.proj.weight  |   16384    |\n",
      "|  sa1_attention.proj.bias   |    128     |\n",
      "|   sa2.mlp_convs.0.weight   |   16768    |\n",
      "|    sa2.mlp_convs.0.bias    |    128     |\n",
      "|   sa2.mlp_convs.1.weight   |   32768    |\n",
      "|    sa2.mlp_convs.1.bias    |    256     |\n",
      "|    sa2.mlp_bns.0.weight    |    128     |\n",
      "|     sa2.mlp_bns.0.bias     |    128     |\n",
      "|    sa2.mlp_bns.1.weight    |    256     |\n",
      "|     sa2.mlp_bns.1.bias     |    256     |\n",
      "| sa2_attention.query.weight |   65536    |\n",
      "|  sa2_attention.query.bias  |    256     |\n",
      "|  sa2_attention.key.weight  |   65536    |\n",
      "|   sa2_attention.key.bias   |    256     |\n",
      "| sa2_attention.value.weight |   65536    |\n",
      "|  sa2_attention.value.bias  |    256     |\n",
      "| sa2_attention.proj.weight  |   65536    |\n",
      "|  sa2_attention.proj.bias   |    256     |\n",
      "|   sa3.mlp_convs.0.weight   |   66304    |\n",
      "|    sa3.mlp_convs.0.bias    |    256     |\n",
      "|   sa3.mlp_convs.1.weight   |   131072   |\n",
      "|    sa3.mlp_convs.1.bias    |    512     |\n",
      "|   sa3.mlp_convs.2.weight   |   524288   |\n",
      "|    sa3.mlp_convs.2.bias    |    1024    |\n",
      "|    sa3.mlp_bns.0.weight    |    256     |\n",
      "|     sa3.mlp_bns.0.bias     |    256     |\n",
      "|    sa3.mlp_bns.1.weight    |    512     |\n",
      "|     sa3.mlp_bns.1.bias     |    512     |\n",
      "|    sa3.mlp_bns.2.weight    |    1024    |\n",
      "|     sa3.mlp_bns.2.bias     |    1024    |\n",
      "| sa3_attention.query.weight |  1048576   |\n",
      "|  sa3_attention.query.bias  |    1024    |\n",
      "|  sa3_attention.key.weight  |  1048576   |\n",
      "|   sa3_attention.key.bias   |    1024    |\n",
      "| sa3_attention.value.weight |  1048576   |\n",
      "|  sa3_attention.value.bias  |    1024    |\n",
      "| sa3_attention.proj.weight  |  1048576   |\n",
      "|  sa3_attention.proj.bias   |    1024    |\n",
      "|  light_head.conv1.weight   |   131072   |\n",
      "|   light_head.conv1.bias    |    128     |\n",
      "|  light_head.conv2.weight   |    1152    |\n",
      "|   light_head.conv2.bias    |     9      |\n",
      "|   light_head.bn1.weight    |    128     |\n",
      "|    light_head.bn1.bias     |    128     |\n",
      "|   light_head.bn2.weight    |     9      |\n",
      "|    light_head.bn2.bias     |     9      |\n",
      "+----------------------------+------------+\n",
      "Total Trainable Params: 5446939\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "total_params = 0\n",
    "for name, parameter in model.named_parameters():\n",
    "    if not parameter.requires_grad: continue\n",
    "    params = parameter.numel()\n",
    "    table.add_row([name, params])\n",
    "    total_params+=params\n",
    "print(table)\n",
    "print(f\"Total Trainable Params: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1086a6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số mẫu trong dataset: 24709\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset = KittiObjectDataset(all_objs, num_points=NUM_POINTS)\n",
    "\n",
    "len_dataset = len(dataset)\n",
    "print(f\"Tổng số mẫu trong dataset: {len_dataset}\")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset,\n",
    "                                          [round(0.7*len_dataset), round(0.1*len_dataset), round(0.2*len_dataset)],\n",
    "                                          generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0ddf3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024, 3])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    points, labels = batch\n",
    "    print(points.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34c22859",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "best_loss= np.inf\n",
    "\n",
    "def training_loop(epochs, model, train_loader,val_dataloader, optimizer, criterion,num_points):\n",
    "    \"\"\"\n",
    "    Hàm huấn luyện mô hình PointNet++ với dữ liệu point cloud từ KITTI.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        epoch_train_loss = []\n",
    "        epoch_train_acc = []\n",
    "\n",
    "        for points, labels in train_loader:\n",
    "            points, labels = points.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(points)  # No need for reshaping\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            epoch_train_loss.append(loss.cpu().item())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            total += labels.size(0)\n",
    "\n",
    "            accuracy = float(total.item() / correct)\n",
    "            epoch_train_acc.append(accuracy)\n",
    "\n",
    "        epoch_test_loss = []\n",
    "        epoch_test_acc = []\n",
    "\n",
    "        for points, labels in val_dataloader:\n",
    "            points, labels = points.to(device), labels.to(device)\n",
    "\n",
    "            model = model.eval()\n",
    "            outputs = model(points)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_test_loss.append(loss.cpu().item())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            accuracy = float(correct / labels.size(0))\n",
    "            epoch_test_acc.append(accuracy)\n",
    "    print('Epoch %s: train loss: %s, val loss: %f, train accuracy: %s,  val accuracy: %f'\n",
    "              % (epoch,\n",
    "                round(np.mean(epoch_train_loss), 4),\n",
    "                round(np.mean(epoch_test_loss), 4),\n",
    "                round(np.mean(epoch_train_acc), 4),\n",
    "                round(np.mean(epoch_test_acc), 4)))\n",
    "    if np.mean(test_loss) < best_loss:\n",
    "        state = {\n",
    "            'model':model.state_dict(),\n",
    "            'optimizer':optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(state, os.path.join('checkpoints', '3DKitti_checkpoint_%s.pth' % (num_points)))\n",
    "        best_loss=np.mean(test_loss)\n",
    "    train_loss.append(np.mean(epoch_train_loss))\n",
    "    test_loss.append(np.mean(epoch_test_loss))\n",
    "    train_acc.append(np.mean(epoch_train_acc))\n",
    "    test_acc.append(np.mean(epoch_test_acc))\n",
    "    return train_loss, train_acc,test_loss,test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f156384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:40<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_loss, train_acc,test_loss,test_acc= \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNUM_POINTS\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mtraining_loop\u001b[39m\u001b[34m(epochs, model, train_loader, val_dataloader, optimizer, criterion, num_points)\u001b[39m\n\u001b[32m     24\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     26\u001b[39m epoch_train_loss.append(loss.cpu().item())\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m optimizer.step()\n\u001b[32m     31\u001b[39m _, predicted = torch.max(outputs, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\Python\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\Python\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\Python\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_loss, train_acc,test_loss,test_acc= training_loop(EPOCHS, model, train_dataloader,val_dataloader, optimizer, criterion,NUM_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd6d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_loss, test_loss, save_to_file=None):\n",
    "    fig = plt.figure()\n",
    "    epochs = len(train_loss)\n",
    "    plt.plot(range(epochs), train_loss, 'b', label='Training loss')\n",
    "    plt.plot(range(epochs), test_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    if save_to_file:\n",
    "        fig.savefig(save_to_file,dpi=200)\n",
    "\n",
    "def plot_accuracy(train_acc, test_acc, save_to_file=None):\n",
    "    fig = plt.figure()\n",
    "    epochs = len(train_acc)\n",
    "    plt.plot(range(epochs), train_acc, 'b', label='Training accuracy')\n",
    "    plt.plot(range(epochs), test_acc, 'r', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    if save_to_file:\n",
    "        fig.savefig(save_to_file,dpi=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_loss, test_loss, save_to_file=False)\n",
    "plot_accuracy(train_acc, test_acc, save_to_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79107082",
   "metadata": {},
   "source": [
    "<h2> Testing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure model is in evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "# # Initialize a list to store predictions\n",
    "# predictions = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for points in test_loader:\n",
    "#         points = points.to(device)  # Send points to GPU if available\n",
    "#         outputs = model(points)  # Get predictions\n",
    "#         _, predicted_classes = torch.max(outputs, 1)  # Predicted class indices\n",
    "#         predictions.append(predicted_classes.cpu().numpy())  # Store predictions\n",
    "\n",
    "# # Flatten predictions into a single array\n",
    "# predictions = np.concatenate(predictions, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
